apiVersion: core.k8sgpt.ai/v1alpha1
kind: K8sGPT
metadata:
  name: k8sgpt-openai
spec:
  ai:
    enabled: true
    model: gpt-4o-mini
    backend: openai
    secret:
      name: openai-api
      key: openai-api-key
    backOff:
      # Enable backOff to implement rate limiting and avoid quota issues
      enabled: true
      # Increase maxRetries to give more attempts with exponential backoff
      maxRetries: 10
      # Add maximum delay between retries (in milliseconds)
      maxDelayMs: 30000
    # anonymized: false
    # language: english
    # proxyEndpoint: https://10.255.30.150 # use proxyEndpoint to setup backend through an HTTP/HTTPS proxy
    # anonymized	<boolean>
    # autoRemediation	<Object>
    # backOff	<Object>
    # backend	<string> -required-
    #   enum: ibmwatsonxai, openai, localai, azureopenai, ....
    # baseUrl	<string>
    # enabled	<boolean>
    # engine	<string>
    # language	<string>
    # maxTokens	<string>
    # model	<string>
    # providerId	<string>
    # proxyEndpoint	<string>
    # region	<string>
    # secret	<Object>
    # topk	<string>
  noCache: false
  repository: ghcr.io/k8sgpt-ai/k8sgpt
  version: v0.3.48
  # Set a schedule for analysis instead of continuous analysis
  filters:
    # Add specific filters to limit resources being analyzed
    - Deployment
    - StatefulSet
    - Pod
  # Enable caching to reduce API calls
  noCache: false
  #integrations:
  # trivy:
  #  enabled: true
  #  namespace: trivy-system
  # sink:
  #   type: slack
  #   webhook: <webhook-url> # use the sink secret if you want to keep your webhook url private
  #   secret:
  #     name: slack-webhook
  #     key: url
  #extraOptions:
  #   backstage:
  #     enabled: true
