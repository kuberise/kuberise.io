{{- if .Values.enable_ollama }}
# This template will only be rendered when enable_openai is set to true
apiVersion: core.k8sgpt.ai/v1alpha1
kind: K8sGPT
metadata:
  name: k8sgpt-ollama
spec:
  ai:
    enabled: true
    model: llama3
    backend: localai
    baseUrl: http://ollama.ollama.svc.cluster.local:11434/v1
    # Set the model to use for analysis
    backOff:
      # Enable backOff to implement rate limiting and avoid quota issues
      enabled: true
      # Increase maxRetries to give more attempts with exponential backoff
      maxRetries: 60
  repository: ghcr.io/k8sgpt-ai/k8sgpt
  version: v0.3.41
  # Set a schedule for analysis instead of continuous analysis
  filters:
    # Add specific filters to limit resources being analyzed
    - Deployment
    - StatefulSet
    - Pod
    - Secret
    - Ingress
  # Enable caching to reduce API calls
  noCache: false
{{- end }}
