{{- if .Values.enable_ollama }}
# This template will only be rendered when enable_openai is set to true
apiVersion: core.k8sgpt.ai/v1alpha1
kind: K8sGPT
metadata:
  name: k8sgpt-ollama
spec:
  ai:
    enabled: true
    model: deepseek-r1:1.5b
    backend: localai
    baseUrl: http://ollama.ollama.svc.cluster.local:11434/
    backOff:
      # Enable backOff to implement rate limiting and avoid quota issues
      enabled: true
      # Increase maxRetries to give more attempts with exponential backoff
      maxRetries: 10
    # anonymized: false
    # language: english
    # proxyEndpoint: https://10.255.30.150 # use proxyEndpoint to setup backend through an HTTP/HTTPS proxy
    # anonymized	<boolean>
    # autoRemediation	<Object>
    # backOff	<Object>
    # backend	<string> -required-
    # options: openai, localai, ollama, azureopenai, cohere, amazonbedrock, amazonsagemaker, google, huggingface, noopai, googlevertexai, ibmwatsonxai
    # baseUrl	<string>
    # enabled	<boolean>
    # engine	<string>
    # language	<string>
    # maxTokens	<string>
    # model	<string>
    # providerId	<string>
    # proxyEndpoint	<string>
    # region	<string>
    # secret	<Object>
    # topk	<string>
  noCache: false
  repository: ghcr.io/k8sgpt-ai/k8sgpt
  version: v0.3.48
  # Set a schedule for analysis instead of continuous analysis
  filters:
    # Add specific filters to limit resources being analyzed
    - Deployment
    - StatefulSet
    - Pod
    - Secret
    - Ingress
  # Enable caching to reduce API calls
  noCache: false
  #integrations:
  # trivy:
  #  enabled: true
  #  namespace: trivy-system
  # sink:
  #   type: slack
  #   webhook: <webhook-url> # use the sink secret if you want to keep your webhook url private
  #   secret:
  #     name: slack-webhook
  #     key: url
  #extraOptions:
  #   backstage:
  #     enabled: true
{{- end }}
