#!/bin/bash

set -euo pipefail

KR_VERSION="0.4.0"

# ── Constants ──────────────────────────────────────────────────────

readonly ARGOCD_CHART_VERSION="9.4.2"
readonly ARGOCD_CHART_REPO="https://argoproj.github.io/argo-helm"
readonly CILIUM_CHART_VERSION="1.19.0"
readonly CILIUM_CHART_REPO="https://helm.cilium.io/"

readonly NAMESPACES=(
  argocd
  postgres
  keycloak
  backstage
  monitoring
  cert-manager
  external-dns
  pgadmin
  gitea
  k8sgpt
)

readonly CA_BUNDLE_NAMESPACES=(
  pgadmin
  monitoring
  argocd
  keycloak
  backstage
  postgres
  cert-manager
  external-dns
)

readonly PG_APP_USERNAME="application"

# All namespaces that kuberise may create (via kr init or ArgoCD apps).
# Excludes kube-system since it is a core Kubernetes namespace.
readonly KUBERISE_NAMESPACES=(
  # Platform Core
  argocd
  backstage
  gitea
  hello
  ingresses
  raw
  teams-namespaces

  # Data Services
  postgres
  pgadmin
  redis
  minio
  object-storage

  # Network Services
  metallb
  external-dns
  external-dns-sigs
  internal-dns
  ingress-nginx-external
  ingress-nginx-internal

  # Security & Auth
  keycloak
  cert-manager
  kyverno
  external-secrets
  sealed-secrets
  vault
  secrets-manager
  neuvector
  trivy-system

  # Monitoring
  monitoring

  # AI Tools
  ollama
  k8sgpt

  # CI/CD
  keda
  tekton-operator
  tekton-pipelines

  # Multi-cluster / management
  cattle-system
  vcluster

  # Example applications
  frontend
  backend
  opencost
)

# Helm releases installed in kube-system by kr init (not managed by ArgoCD).
# Cilium is intentionally NOT uninstalled - it is the CNI (networking layer).
readonly KUBE_SYSTEM_HELM_RELEASES=()

# ── Embedded Resources ────────────────────────────────────────────

# ISRG Root X1 (Let's Encrypt) certificate
read -r -d '' LETSENCRYPT_CRT <<'CERTEOF' || true
-----BEGIN CERTIFICATE-----
MIIFazCCA1OgAwIBAgIRAIIQz7DSQONZRGPgu2OCiwAwDQYJKoZIhvcNAQELBQAw
TzELMAkGA1UEBhMCVVMxKTAnBgNVBAoTIEludGVybmV0IFNlY3VyaXR5IFJlc2Vh
cmNoIEdyb3VwMRUwEwYDVQQDEwxJU1JHIFJvb3QgWDEwHhcNMTUwNjA0MTEwNDM4
WhcNMzUwNjA0MTEwNDM4WjBPMQswCQYDVQQGEwJVUzEpMCcGA1UEChMgSW50ZXJu
ZXQgU2VjdXJpdHkgUmVzZWFyY2ggR3JvdXAxFTATBgNVBAMTDElTUkcgUm9vdCBY
MTCCAiIwDQYJKoZIhvcNAQEBBQADggIPADCCAgoCggIBAK3oJHP0FDfzm54rVygc
h77ct984kIxuPOZXoHj3dcKi/vVqbvYATyjb3miGbESTtrFj/RQSa78f0uoxmyF+
0TM8ukj13Xnfs7j/EvEhmkvBioZxaUpmZmyPfjxwv60pIgbz5MDmgK7iS4+3mX6U
A5/TR5d8mUgjU+g4rk8Kb4Mu0UlXjIB0ttov0DiNewNwIRt18jA8+o+u3dpjq+sW
T8KOEUt+zwvo/7V3LvSye0rgTBIlDHCNAymg4VMk7BPZ7hm/ELNKjD+Jo2FR3qyH
B5T0Y3HsLuJvW5iB4YlcNHlsdu87kGJ55tukmi8mxdAQ4Q7e2RCOFvu396j3x+UC
B5iPNgiV5+I3lg02dZ77DnKxHZu8A/lJBdiB3QW0KtZB6awBdpUKD9jf1b0SHzUv
KBds0pjBqAlkd25HN7rOrFleaJ1/ctaJxQZBKT5ZPt0m9STJEadao0xAH0ahmbWn
OlFuhjuefXKnEgV4We0+UXgVCwOPjdAvBbI+e0ocS3MFEvzG6uBQE3xDk3SzynTn
jh8BCNAw1FtxNrQHusEwMFxIt4I7mKZ9YIqioymCzLq9gwQbooMDQaHWBfEbwrbw
qHyGO0aoSCqI3Haadr8faqU9GY/rOPNk3sgrDQoo//fb4hVC1CLQJ13hef4Y53CI
rU7m2Ys6xt0nUW7/vGT1M0NPAgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBBjAPBgNV
HRMBAf8EBTADAQH/MB0GA1UdDgQWBBR5tFnme7bl5AFzgAiIyBpY9umbbjANBgkq
hkiG9w0BAQsFAAOCAgEAVR9YqbyyqFDQDLHYGmkgJykIrGF1XIpu+ILlaS/V9lZL
ubhzEFnTIZd+50xx+7LSYK05qAvqFyFWhfFQDlnrzuBZ6brJFe+GnY+EgPbk6ZGQ
3BebYhtF8GaV0nxvwuo77x/Py9auJ/GpsMiu/X1+mvoiBOv/2X/qkSsisRcOj/KK
NFtY2PwByVS5uCbMiogziUwthDyC3+6WVwW6LLv3xLfHTjuCvjHIInNzktHCgKQ5
ORAzI4JMPJ+GslWYHb4phowim57iaztXOoJwTdwJx4nLCgdNbOhdjsnvzqvHu7Ur
TkXWStAmzOVyyghqpZXjFaH3pO3JLF+l+/+sKAIuvtd7u+Nxe5AW0wdeRlN8NwdC
jNPElpzVmbUq4JUagEiuTDkHzsxHpFKVK7q4+63SM1N95R1NbdWhscdCb+ZAJzVc
oyi3B43njTOQ5yOf+1CceWxG1bQVs5ZufpsMljq4Ui0/1lvh+wjChP4kqKOJ2qxq
4RgqsahDYVvTH9w7jXbyLeiNdd8XM2w9U/t7y0Ff/9yi0GE44Za4rF2LN9d11TPA
mRGunUHBcnWEvgJBQl9nJEiU0Zsnvgc/ubhPgXRR4Xq37Z0j4r7g1SgEEzwxA57d
emyPxgcYxn/eR44/KJ4EBs+lVDR3veyJm+kXQ99b21/+jh5Xos1AnX5iItreGCc=
-----END CERTIFICATE-----
CERTEOF

# ── Logging ────────────────────────────────────────────────────────

if [[ -t 1 ]] && [[ -z "${NO_COLOR:-}" ]]; then
  readonly C_RESET='\033[0m'
  readonly C_INFO='\033[0;36m'   # cyan
  readonly C_WARN='\033[0;33m'   # yellow
  readonly C_ERROR='\033[0;31m'  # red
  readonly C_STEP='\033[1;36m'   # bold cyan
else
  readonly C_RESET='' C_INFO='' C_WARN='' C_ERROR='' C_STEP=''
fi

function log_info()  { echo -e "${C_INFO}[INFO]${C_RESET}  $*"; }
function log_warn()  { echo -e "${C_WARN}[WARN]${C_RESET}  $*" >&2; }
function log_error() { echo -e "${C_ERROR}[ERROR]${C_RESET} $*" >&2; }
function log_step()  { echo ""; echo -e "${C_STEP}── $* ──${C_RESET}"; }

# ── Cleanup ────────────────────────────────────────────────────────

TEMP_FILES=()
TEMP_DIRS=()

function cleanup() {
  if [[ ${#TEMP_FILES[@]} -gt 0 ]]; then
    for f in "${TEMP_FILES[@]}"; do
      rm -f "$f"
    done
  fi
  if [[ ${#TEMP_DIRS[@]} -gt 0 ]]; then
    for d in "${TEMP_DIRS[@]}"; do
      rm -rf "$d"
    done
  fi
}
trap cleanup EXIT

function make_temp_file() {
  local f
  f=$(mktemp)
  TEMP_FILES+=("$f")
  echo "$f"
}

# ── Utility Functions ──────────────────────────────────────────────

# Inject a token into an HTTPS git URL for authenticated cloning.
# Converts https://github.com/org/repo.git to https://x:TOKEN@github.com/org/repo.git
function inject_token_into_url() {
  local url=$1
  local token=$2
  echo "$url" | sed "s|https://|https://x:${token}@|"
}

# Fetch kuberise.yaml from a remote git repo when it's not found locally.
# Uses shallow clone + sparse checkout to minimize bandwidth.
#
# Arguments:
#   $1 - Git repo URL (e.g., https://github.com/org/client-webshop.git)
#   $2 - Git revision (branch, tag, or SHA; default: HEAD)
#   $3 - Git token for private repos (optional)
#
# Prints the path to the fetched kuberise.yaml on stdout.
# Returns non-zero if fetch fails.
function fetch_kuberise_yaml() {
  local repo_url=$1
  local revision=${2:-HEAD}
  local token=${3:-}

  log_info "kuberise.yaml not found locally, fetching from $repo_url..." >&2

  local tmp_dir
  tmp_dir=$(mktemp -d)
  TEMP_DIRS+=("$tmp_dir")

  local clone_url="$repo_url"
  if [[ -n "$token" ]]; then
    clone_url=$(inject_token_into_url "$clone_url" "$token")
  fi

  # Build clone args: use --branch only for non-HEAD revisions
  local clone_args=(--depth 1 --single-branch --no-checkout)
  if [[ "$revision" != "HEAD" ]]; then
    clone_args+=(--branch "$revision")
  fi

  if ! git clone "${clone_args[@]}" "$clone_url" "$tmp_dir" 2>/dev/null; then
    log_error "Failed to clone $repo_url (revision: $revision)"
    return 1
  fi

  if ! (cd "$tmp_dir" && git checkout HEAD -- kuberise.yaml) 2>/dev/null; then
    log_error "kuberise.yaml not found in $repo_url (revision: $revision)"
    return 1
  fi

  echo "$tmp_dir/kuberise.yaml"
}

function generate_random_secret() {
  # Generate a 32-character alphanumeric string.
  # Uses parameter expansion instead of piping to head to avoid SIGPIPE with pipefail.
  local raw
  raw=$(openssl rand -base64 48)
  raw="${raw//[^a-zA-Z0-9]/}"
  echo "${raw:0:32}"
}

# ── Kubernetes Helper Functions ────────────────────────────────────

# Filter out "unchanged" lines from kubectl apply output to reduce noise on re-runs.
# The "|| true" prevents grep from returning exit code 1 when everything is unchanged,
# which would otherwise abort the script under set -eo pipefail.
function filter_unchanged() {
  grep -v ' unchanged$' || true
}

# In dry-run mode, print the manifest from stdin instead of applying it.
# In normal mode, apply the manifest via kubectl.
function apply_manifest() {
  if [[ "${KR_DRY_RUN:-false}" == true ]]; then
    cat
    echo "---"
  else
    kubectl apply "$@" | filter_unchanged
  fi
}

function create_namespace() {
  local context=$1
  local namespace=$2
  kubectl create namespace "$namespace" \
    --context "$context" \
    --dry-run=client -o yaml | \
    kubectl apply --context "$context" -f - | filter_unchanged
}

function create_secret() {
  local context=$1
  local namespace=$2
  local secret_name=$3
  shift 3
  # Remaining arguments (--from-literal, --type, etc.) are passed directly to kubectl
  kubectl create secret generic "$secret_name" \
    --context "$context" \
    -n "$namespace" \
    "$@" \
    --dry-run=client -o yaml | \
    apply_manifest --context "$context" -n "$namespace" -f -
}

function label_secret() {
  local context=$1
  local namespace=$2
  local secret_name=$3
  local label=$4
  if [[ "${KR_DRY_RUN:-false}" == true ]]; then
    log_info "[dry-run] Would label secret $namespace/$secret_name with $label"
    return
  fi
  kubectl label secret "$secret_name" "$label" \
    --context "$context" -n "$namespace" --overwrite
}

function secret_exists() {
  local context=$1
  local namespace=$2
  local secret_name=$3
  kubectl get secret "$secret_name" --context "$context" -n "$namespace" &>/dev/null
}

# Retrieves or generates a secret value
#
# This function checks if a secret exists in a namespace:
# - If it doesn't exist, generates a random value
# - If it exists, retrieves the value from the specified key
#
# Arguments:
#   $1 - Kubernetes context
#   $2 - Namespace
#   $3 - Secret name
#   $4 - Key in the secret to retrieve (default: 'password')
#
# Example usage:
#   password=$(get_or_generate_secret "$CONTEXT" "postgres" "database-superuser" "password")
#
# Returns:
#   The secret value (either retrieved or newly generated) on stdout
function get_or_generate_secret() {
  local context=$1
  local namespace=$2
  local secret_name=$3
  local key=${4:-"password"}

  local secret_value
  if ! secret_exists "$context" "$namespace" "$secret_name"; then
    log_info "Generating random value for $secret_name" >&2
    secret_value=$(generate_random_secret)
  else
    log_info "Secret $secret_name already exists, reusing it" >&2
    secret_value=$(kubectl get secret "$secret_name" --context "$context" -n "$namespace" -o jsonpath="{.data.$key}" | base64 -d)
  fi

  echo "$secret_value"
}

# ── Init Functions ─────────────────────────────────────────────────

function create_all_namespaces() {
  local context=$1
  for ns in "${NAMESPACES[@]}"; do
    create_namespace "$context" "$ns"
  done
}

function generate_ca_cert_and_key() {
  local context=$1

  local dir=".env"
  local cert="$dir/ca.crt"
  local key="$dir/ca.key"
  local ca_bundle="$dir/ca-bundle.crt"

  if [ ! -f "$cert" ] || [ ! -f "$key" ]; then
    log_info "CA certificate/key files do not exist. Generating..."
    mkdir -p "$dir"
    openssl req -x509 -newkey rsa:4096 -sha256 -days 3650 -nodes \
      -keyout "$key" -out "$cert" -subj "/CN=ca.kuberise.local CA/O=KUBERISE/C=NL"
    log_info "CA certificate and key generated."
  else
    log_info "CA certificate and key already exist."
  fi

  log_info "Creating CA bundle with self-signed and Let's Encrypt certificates..."
  local letsencrypt_file
  letsencrypt_file=$(make_temp_file)
  echo "$LETSENCRYPT_CRT" > "$letsencrypt_file"
  cat "$cert" "$letsencrypt_file" > "$ca_bundle"

  # Create TLS secret for cert-manager CA issuer
  kubectl create secret tls ca-key-pair-external \
    --cert="$cert" \
    --key="$key" \
    --namespace="cert-manager" \
    --dry-run=client -o yaml | kubectl apply --namespace="cert-manager" --context="$context" -f - | filter_unchanged

  # Create CA bundle ConfigMap in all relevant namespaces
  for ns in "${CA_BUNDLE_NAMESPACES[@]}"; do
    kubectl create configmap ca-bundle \
      --from-file=ca.crt="$ca_bundle" \
      --namespace="$ns" \
      --dry-run=client -o yaml | kubectl apply --namespace="$ns" --context="$context" -f - | filter_unchanged
  done

  log_info "CA bundle created and ConfigMaps updated in all namespaces."
}

function create_database_secrets() {
  local context=$1
  local admin_password=$2

  # PostgreSQL application database
  PG_APP_PASSWORD=$(get_or_generate_secret "$context" "postgres" "database-app" "password")
  create_secret "$context" "postgres" "database-app" \
    --from-literal=dbname=app \
    --from-literal=host=database-rw \
    --from-literal=username="$PG_APP_USERNAME" \
    --from-literal=user="$PG_APP_USERNAME" \
    --from-literal=port=5432 \
    --from-literal=password="$PG_APP_PASSWORD" \
    --type=kubernetes.io/basic-auth

  # PostgreSQL superuser
  PG_SUPERUSER_PASSWORD=$(get_or_generate_secret "$context" "postgres" "database-superuser" "password")
  create_secret "$context" "postgres" "database-superuser" \
    --from-literal=dbname="*" \
    --from-literal=host=database-rw \
    --from-literal=username=postgres \
    --from-literal=user=postgres \
    --from-literal=port=5432 \
    --from-literal=password="$PG_SUPERUSER_PASSWORD" \
    --type=kubernetes.io/basic-auth

  # Gitea admin (password configurable via GITEA_ADMIN_PASSWORD env var)
  local gitea_password="${GITEA_ADMIN_PASSWORD:-$admin_password}"
  create_secret "$context" "gitea" "gitea-admin-secret" \
    --from-literal=username=gitea_admin \
    --from-literal=password="$gitea_password" \
    --from-literal=email=admin@gitea.admin \
    --from-literal=passwordMode=keepUpdated \
    --type=kubernetes.io/basic-auth
}

function create_application_secrets() {
  local context=$1
  local admin_password=$2

  # K8sGPT (optional, requires OPENAI_API_KEY env var)
  if [ -n "${OPENAI_API_KEY:-}" ]; then
    create_secret "$context" "k8sgpt" "openai-api" \
      --from-literal=openai-api-key="$OPENAI_API_KEY"
  fi

  # Keycloak database credentials
  create_secret "$context" "keycloak" "pg-secret" \
    --from-literal=KC_DB_USERNAME="$PG_APP_USERNAME" \
    --from-literal=KC_DB_PASSWORD="$PG_APP_PASSWORD"

  # Keycloak admin
  create_secret "$context" "keycloak" "admin-secret" \
    --from-literal=KEYCLOAK_ADMIN=admin \
    --from-literal=KEYCLOAK_ADMIN_PASSWORD="$admin_password"

  # Backstage database
  create_secret "$context" "backstage" "pg-secret" \
    --from-literal=password="$PG_APP_PASSWORD"

  # Grafana admin
  create_secret "$context" "monitoring" "grafana-admin" \
    --from-literal=admin-user=admin \
    --from-literal=admin-password="$admin_password" \
    --from-literal=ldap-toml=

  # Cloudflare (optional, requires CLOUDFLARE_API_TOKEN env var)
  if [ -n "${CLOUDFLARE_API_TOKEN:-}" ]; then
    create_secret "$context" "external-dns" "cloudflare" \
      --from-literal=cloudflare_api_token="$CLOUDFLARE_API_TOKEN"
    create_secret "$context" "cert-manager" "cloudflare" \
      --from-literal=cloudflare_api_token="$CLOUDFLARE_API_TOKEN"
  fi

  # Keycloak operator access to master realm
  create_secret "$context" "keycloak" "keycloak-access" \
    --from-literal=username=admin \
    --from-literal=password="$admin_password"
}

function configure_oauth2_clients() {
  local context=$1
  local admin_password=$2
  local domain=$3
  local cluster_name=$4

  # Kubernetes OAuth2 client
  log_info "Setting up Kubernetes OAuth2 client secret..."
  local kubernetes_secret
  kubernetes_secret=$(get_or_generate_secret "$context" "keycloak" "kubernetes-oauth2-client-secret" "client-secret")
  create_secret "$context" "keycloak" "kubernetes-oauth2-client-secret" \
    --from-literal=client-secret="$kubernetes_secret"
  configure_oidc_auth "$context" "$kubernetes_secret" "$domain" "$cluster_name"

  # Grafana OAuth2 client
  log_info "Setting up Grafana OAuth2 client secret..."
  local grafana_secret
  grafana_secret=$(get_or_generate_secret "$context" "keycloak" "grafana-oauth2-client-secret" "client-secret")
  create_secret "$context" "keycloak" "grafana-oauth2-client-secret" \
    --from-literal=client-secret="$grafana_secret"
  create_secret "$context" "monitoring" "grafana-oauth2-client-secret" \
    --from-literal=client-secret="$grafana_secret"

  # PGAdmin OAuth2 client
  log_info "Setting up PGAdmin OAuth2 client secret..."
  local pgadmin_secret
  pgadmin_secret=$(get_or_generate_secret "$context" "keycloak" "pgadmin-oauth2-client-secret" "client-secret")
  create_secret "$context" "keycloak" "pgadmin-oauth2-client-secret" \
    --from-literal=client-secret="$pgadmin_secret"
  create_secret "$context" "pgadmin" "pgadmin-oauth2-client-secret" \
    --from-literal=client-secret="$pgadmin_secret"

  # OAuth2-Proxy client (with persistent cookie secret)
  log_info "Setting up OAuth2-Proxy client secret..."
  local oauth2_proxy_secret
  oauth2_proxy_secret=$(get_or_generate_secret "$context" "keycloak" "oauth2-proxy-oauth2-client-secret" "client-secret")
  local cookie_secret
  cookie_secret=$(get_or_generate_secret "$context" "keycloak" "oauth2-proxy-oauth2-client-secret" "cookie-secret")
  if [[ -z "$cookie_secret" ]]; then
    cookie_secret=$(generate_random_secret)
  fi
  create_secret "$context" "keycloak" "oauth2-proxy-oauth2-client-secret" \
    --from-literal=client-secret="$oauth2_proxy_secret" \
    --from-literal=client-id=oauth2-proxy \
    --from-literal=cookie-secret="$cookie_secret"

  # ArgoCD OAuth2 client
  log_info "Setting up ArgoCD OAuth2 client secret..."
  local argocd_secret
  argocd_secret=$(get_or_generate_secret "$context" "keycloak" "argocd-oauth2-client-secret" "client-secret")
  create_secret "$context" "keycloak" "argocd-oauth2-client-secret" \
    --from-literal=client-secret="$argocd_secret"
  local argocd_client_secret_b64
  argocd_client_secret_b64=$(echo -n "$argocd_secret" | base64)
  kubectl patch secret argocd-secret --context "$context" -n argocd --patch "
data:
  oidc.keycloak.clientSecret: $argocd_client_secret_b64
"
}

function configure_oidc_auth() {
  local context=$1
  local client_secret=$2
  local domain=$3
  local cluster_name=$4

  log_info "Configuring OIDC authentication in kubeconfig..."

  local cluster_name_k8s
  cluster_name_k8s=$(kubectl config view -o jsonpath="{.contexts[?(@.name == \"$context\")].context.cluster}")

  local oidc_user="oidc-$cluster_name"
  local oidc_context="oidc-$cluster_name"

  kubectl config set-credentials "$oidc_user" \
    --exec-api-version=client.authentication.k8s.io/v1beta1 \
    --exec-command=kubectl \
    --exec-arg=oidc-login \
    --exec-arg=get-token \
    --exec-arg="--oidc-issuer-url=https://keycloak.$domain/realms/platform" \
    --exec-arg=--oidc-client-id=kubernetes \
    --exec-arg="--oidc-client-secret=$client_secret"

  kubectl config set-context "$oidc_context" \
    --cluster="$cluster_name_k8s" \
    --user="$oidc_user" \
    --namespace=default

  log_info "OIDC configured. Use 'kubectl config use-context $oidc_context' to switch."
}

function install_argocd() {
  local context=$1
  local admin_password=$2
  local domain=$3

  log_info "Installing ArgoCD..."
  local bcrypt_hash
  bcrypt_hash=$(htpasswd -nbBC 10 "" "$admin_password" | tr -d ':\n' | sed 's/$2y/$2a/')

  helm upgrade --install --kube-context "$context" \
    -n argocd --create-namespace --wait \
    --set configs.secret.argocdServerAdminPassword="$bcrypt_hash" \
    --set configs.params.server.insecure=true \
    --set server.service.type=ClusterIP \
    --repo "$ARGOCD_CHART_REPO" --version "$ARGOCD_CHART_VERSION" \
    argocd argo-cd > /dev/null

  log_info "ArgoCD installed. Full configuration (ingress, OIDC, health checks) will be applied via GitOps after 'kr deploy'."
}

function install_cilium() {
  local context=$1

  log_info "Installing Cilium..."

  helm upgrade --install --kube-context "$context" \
    -n kube-system --wait \
    --set operator.replicas=1 \
    --repo "$CILIUM_CHART_REPO" --version "$CILIUM_CHART_VERSION" \
    cilium cilium > /dev/null

  log_info "Cilium installed. Advanced configuration (ClusterMesh, etc.) will be applied via GitOps after 'kr deploy'."
}

# ── Deploy Functions ───────────────────────────────────────────────

function configure_repo_secret() {
  local context=$1
  local secret_name=$2
  local label_name=$3
  local repo_url=$4
  local token=$5

  if [ -n "$token" ]; then
    create_secret "$context" "argocd" "$secret_name" \
      --from-literal=name="$label_name" \
      --from-literal=username=x \
      --from-literal=password="$token" \
      --from-literal=url="$repo_url" \
      --from-literal=type=git
    label_secret "$context" "argocd" "$secret_name" "argocd.argoproj.io/secret-type=repository"
  fi
}

function configure_all_repo_access() {
  local context=$1
  local kuberise_repo=$2
  local kuberise_token=$3
  local client_repo=$4
  local client_token=$5

  # Kuberise (OSS) repo (usually public, token optional)
  configure_repo_secret "$context" "argocd-repo-kuberise" "kuberise" \
    "$kuberise_repo" "$kuberise_token"

  # Client repo
  configure_repo_secret "$context" "argocd-repo-client" "client" \
    "$client_repo" "$client_token"
}

function configure_layer_repo_access() {
  local context=$1
  local layer_name=$2
  local layer_repo=$3
  local layer_token=$4

  configure_repo_secret "$context" "argocd-repo-layer-${layer_name}" "layer-${layer_name}" \
    "$layer_repo" "$layer_token"
}

function create_app_project() {
  local context=$1
  local destination=${2:-https://kubernetes.default.svc}

  apply_manifest --context "$context" -n argocd -f - <<EOF
apiVersion: argoproj.io/v1alpha1
kind: AppProject
metadata:
  name: kuberise
  namespace: argocd
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  sourceRepos:
    - '*'
  destinations:
    - name: '*'
      namespace: '*'
      server: $destination
  clusterResourceWhitelist:
    - group: '*'
      kind: '*'
  namespaceResourceWhitelist:
    - group: '*'
      kind: '*'
EOF
}

function create_layer_app_of_apps() {
  local context=$1
  local cluster_name=$2
  local layer_name=$3
  local layer_repo=$4
  local layer_revision=$5
  local kuberise_repo=$6
  local kuberise_revision=$7
  local client_repo=$8
  local client_revision=$9
  local domain=${10}
  local destination=${11:-https://kubernetes.default.svc}

  # Enabler file uses combined naming: values-{clusterName}-{layerName}.yaml
  local enabler_file="values-${cluster_name}-${layer_name}.yaml"

  # Each layer gets its own ArgoCD Application: app-of-apps-{layer_name}
  # Chart source is always the kuberise (OSS) repo.
  # Layer values come from the layer repo ($layer ref).
  # Enabler files come from the client repo ($client ref).

  # No resources-finalizer here: deleting app-of-apps with a finalizer triggers
  # a deep cascade that is slow and fragile. The uninstall handles cleanup explicitly.
  apply_manifest --context "$context" -n argocd -f - <<EOF
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: app-of-apps-$layer_name
  namespace: argocd
  labels:
    team: platform
spec:
  project: kuberise
  sources:
    - repoURL: $kuberise_repo
      targetRevision: $kuberise_revision
      path: ./app-of-apps
      helm:
        ignoreMissingValueFiles: true
        valueFiles:
          - \$layer/app-of-apps/values-base.yaml
          - \$client/app-of-apps/$enabler_file
        parameters:
          - name: global.argoProject
            value: kuberise
          - name: global.spec.source.repoURL
            value: $layer_repo
          - name: global.spec.source.targetRevision
            value: $layer_revision
          - name: global.spec.values.repoURL
            value: $client_repo
          - name: global.spec.values.targetRevision
            value: $client_revision
          - name: global.spec.defaults.repoURL
            value: $layer_repo
          - name: global.spec.defaults.targetRevision
            value: $layer_revision
          - name: global.clusterName
            value: $cluster_name
          - name: global.domain
            value: $domain
          - name: global.spec.destination.server
            value: $destination
    - repoURL: $layer_repo
      targetRevision: $layer_revision
      ref: layer
    - repoURL: $client_repo
      targetRevision: $client_revision
      ref: client
  destination:
    server: $destination
    namespace: argocd
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
EOF
}

# ── Uninstall Functions ────────────────────────────────────────────

# Wrap kubectl for uninstall: bounded timeout and forced context
function kubectl_uninstall() {
  local timeout="${KUBECTL_REQUEST_TIMEOUT:-15s}"
  command kubectl --request-timeout="$timeout" --context "$UNINSTALL_CONTEXT" "$@"
}

function collect_uninstall_namespaces() {
  local candidates
  local discovered

  candidates=("${KUBERISE_NAMESPACES[@]}")
  discovered=$(kubectl_uninstall get applications -n argocd \
    -o jsonpath='{range .items[*]}{.spec.destination.namespace}{"\n"}{range .status.resources[*]}{.namespace}{"\n"}{end}{end}' 2>/dev/null || true)

  if [[ -n "$discovered" ]]; then
    while IFS= read -r ns; do
      [[ -z "$ns" ]] && continue
      case "$ns" in
        kube-system|kube-public|kube-node-lease|default)
          continue
          ;;
      esac
      candidates+=("$ns")
    done <<< "$discovered"
  fi

  UNINSTALL_NAMESPACES=()
  while IFS= read -r ns; do
    [[ -z "$ns" ]] && continue
    UNINSTALL_NAMESPACES+=("$ns")
  done < <(printf '%s\n' "${candidates[@]}" | awk 'NF' | sort -u)
}

function get_application_names() {
  kubectl_uninstall get applications -n argocd -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || echo ""
}

function remove_app_of_apps() {
  log_info "Deleting app-of-apps ArgoCD application(s)..."
  local aoa_apps
  aoa_apps=$(get_application_names)
  for app in $aoa_apps; do
    if [[ "$app" == app-of-apps* ]]; then
      log_info "  Deleting $app"
      kubectl_uninstall delete -n argocd application "$app" --ignore-not-found 2>/dev/null || true
    fi
  done

  delete_all_applications

  log_info "Deleting ArgoCD project..."
  kubectl_uninstall patch appproject kuberise -n argocd \
    --type merge -p '{"metadata":{"finalizers":null}}' 2>/dev/null || true
  kubectl_uninstall delete -n argocd appproject kuberise --ignore-not-found 2>/dev/null || true
}

function delete_all_applications() {
  local apps
  apps=$(get_application_names)

  if [[ -z "$apps" ]]; then
    log_info "No remaining ArgoCD applications to delete."
    return
  fi

  local count
  count=$(echo "$apps" | wc -w | tr -d ' ')
  log_info "Deleting all $count ArgoCD application(s) at once..."
  kubectl_uninstall delete applications --all -n argocd --wait=false 2>/dev/null || true

  local timeout=30
  local elapsed=0
  local interval=5

  while [[ $elapsed -lt $timeout ]]; do
    apps=$(get_application_names)

    if [[ -z "$apps" ]]; then
      log_info "All ArgoCD applications deleted successfully."
      return
    fi

    count=$(echo "$apps" | wc -w | tr -d ' ')
    log_info "Waiting for $count application(s) to finish deleting... (${elapsed}s/${timeout}s)"
    sleep "$interval"
    elapsed=$((elapsed + interval))
  done

  apps=$(get_application_names)
  count=$(echo "$apps" | wc -w | tr -d ' ')
  log_warn "$count application(s) still stuck after ${timeout}s. Clearing stuck resources..."

  for app in $apps; do
    clear_stuck_managed_resources "$app"
  done

  log_info "Waiting for ArgoCD to process resource cleanup..."
  sleep 10

  apps=$(get_application_names)
  for app in $apps; do
    log_warn "  Force-clearing ArgoCD finalizers on application: $app"
    kubectl_uninstall patch application "$app" -n argocd \
      --type merge -p '{"metadata":{"finalizers":null}}' 2>/dev/null || true
  done

  sleep 5
  apps=$(get_application_names)
  if [[ -z "$apps" ]]; then
    log_info "All ArgoCD applications deleted successfully."
  else
    log_warn "Some applications may still be terminating."
  fi
}

function clear_stuck_managed_resources() {
  local app_name=$1
  log_info "  Checking managed resources for stuck application: $app_name"

  local resources
  resources=$(kubectl_uninstall get application "$app_name" -n argocd \
    -o jsonpath='{range .status.resources[*]}{.group}{"\t"}{.kind}{"\t"}{.namespace}{"\t"}{.name}{"\n"}{end}' 2>/dev/null || echo "")

  while IFS=$'\t' read -r group kind ns name; do
    [[ -z "$kind" || -z "$name" ]] && continue
    if [[ -z "$group" ]]; then
      [[ "$kind" != "PersistentVolumeClaim" && "$kind" != "PersistentVolume" ]] && continue
    fi

    local resource_ref="${kind}.${group}"
    [[ -z "$group" ]] && resource_ref="$kind"

    local deletion_ts
    if [[ -z "$ns" ]]; then
      deletion_ts=$(kubectl_uninstall get "$resource_ref" "$name" \
        -o jsonpath='{.metadata.deletionTimestamp}' 2>/dev/null || echo "")
    else
      deletion_ts=$(kubectl_uninstall get "$resource_ref" "$name" -n "$ns" \
        -o jsonpath='{.metadata.deletionTimestamp}' 2>/dev/null || echo "")
    fi

    if [[ -n "$deletion_ts" ]]; then
      if [[ -z "$ns" ]]; then
        log_info "    Clearing finalizers on ${kind}/${name}"
        kubectl_uninstall patch "$resource_ref" "$name" \
          --type merge -p '{"metadata":{"finalizers":null}}' 2>/dev/null || true
      else
        log_info "    Clearing finalizers on ${kind}/${name} in namespace ${ns}"
        kubectl_uninstall patch "$resource_ref" "$name" -n "$ns" \
          --type merge -p '{"metadata":{"finalizers":null}}' 2>/dev/null || true
      fi
    fi
  done <<< "$resources"
}

function clear_namespace_finalizers() {
  local ns=$1
  log_warn "  Force-clearing spec.finalizers on namespace: $ns"
  local payload='{"apiVersion":"v1","kind":"Namespace","metadata":{"name":"'"$ns"'"},"spec":{"finalizers":[]}}'
  echo "$payload" | kubectl_uninstall replace --raw "/api/v1/namespaces/$ns/finalize" -f - 2>/dev/null || true
}

function clear_stuck_pvcs_and_pvs() {
  local count_pvc=0
  local count_pv=0

  for ns in "${UNINSTALL_NAMESPACES[@]}"; do
    local names
    names=$(kubectl_uninstall get pvc -n "$ns" -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || echo "")
    for pvc in $names; do
      [[ -z "$pvc" ]] && continue
      local dt
      dt=$(kubectl_uninstall get pvc "$pvc" -n "$ns" -o jsonpath='{.metadata.deletionTimestamp}' 2>/dev/null || echo "")
      if [[ -n "$dt" ]]; then
        log_info "  Clearing finalizers on PVC $ns/$pvc"
        kubectl_uninstall patch pvc "$pvc" -n "$ns" --type merge -p '{"metadata":{"finalizers":null}}' 2>/dev/null || true
        count_pvc=$((count_pvc + 1))
      fi
    done
  done

  local pv_names
  pv_names=$(kubectl_uninstall get pv -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || echo "")
  for pv in $pv_names; do
    [[ -z "$pv" ]] && continue
    local dt
    dt=$(kubectl_uninstall get pv "$pv" -o jsonpath='{.metadata.deletionTimestamp}' 2>/dev/null || echo "")
    if [[ -n "$dt" ]]; then
      log_info "  Clearing finalizers on PV $pv"
      kubectl_uninstall patch pv "$pv" --type merge -p '{"metadata":{"finalizers":null}}' 2>/dev/null || true
      count_pv=$((count_pv + 1))
    fi
  done

  if [[ $count_pvc -gt 0 || $count_pv -gt 0 ]]; then
    log_info "Cleared finalizers on $count_pvc PVC(s) and $count_pv PV(s) stuck in Terminating."
  fi
}

function uninstall_argocd() {
  log_info "Uninstalling ArgoCD via Helm..."
  helm uninstall argocd --kube-context "$UNINSTALL_CONTEXT" -n argocd 2>/dev/null || true
}

function uninstall_kube_system_releases() {
  for release in "${KUBE_SYSTEM_HELM_RELEASES[@]}"; do
    log_info "Uninstalling Helm release '$release' from kube-system..."
    helm uninstall "$release" --kube-context "$UNINSTALL_CONTEXT" -n kube-system 2>/dev/null || true
  done
}

function delete_orphaned_webhooks() {
  local type=$1
  local label=$2
  local names
  names=$(kubectl_uninstall get "$type" -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || echo "")

  for name in $names; do
    local svc_namespaces
    svc_namespaces=$(kubectl_uninstall get "$type" "$name" \
      -o jsonpath='{range .webhooks[*]}{.clientConfig.service.namespace}{"\n"}{end}' 2>/dev/null || echo "")
    for ns in "${UNINSTALL_NAMESPACES[@]}"; do
      if echo "$svc_namespaces" | grep -qxF "$ns"; then
        log_info "  Deleting $label: $name (references namespace $ns)"
        kubectl_uninstall delete "$type" "$name" --ignore-not-found 2>/dev/null || true
        break
      fi
    done
  done
}

function cleanup_cluster_scoped_resources() {
  log_info "Cleaning up orphaned webhook configurations..."
  delete_orphaned_webhooks validatingwebhookconfiguration "ValidatingWebhookConfiguration"
  delete_orphaned_webhooks mutatingwebhookconfiguration "MutatingWebhookConfiguration"
}

function delete_namespaces() {
  for ns in "${UNINSTALL_NAMESPACES[@]}"; do
    if kubectl_uninstall get namespace "$ns" &>/dev/null; then
      log_info "Deleting namespace: $ns"
      kubectl_uninstall delete namespace "$ns" --wait=false 2>/dev/null || true
    fi
  done
}

function cleanup_stuck_namespaces() {
  log_info "Clearing finalizers on any PVCs and PVs stuck in Terminating..."
  clear_stuck_pvcs_and_pvs

  local stuck_namespaces=()
  for ns in "${UNINSTALL_NAMESPACES[@]}"; do
    local phase
    phase=$(kubectl_uninstall get namespace "$ns" \
      -o jsonpath='{.status.phase}' 2>/dev/null || echo "")
    if [[ "$phase" == "Terminating" ]]; then
      stuck_namespaces+=("$ns")
    fi
  done

  if [[ ${#stuck_namespaces[@]} -eq 0 ]]; then
    log_info "All namespaces deleted or deleting normally."
    return
  fi

  local count=${#stuck_namespaces[@]}
  log_info "Waiting for $count namespace(s) still in Terminating state..."

  local timeout=60
  local elapsed=0
  local interval=5

  while [[ $elapsed -lt $timeout ]]; do
    local still_stuck=()
    for ns in "${stuck_namespaces[@]}"; do
      local phase
      phase=$(kubectl_uninstall get namespace "$ns" \
        -o jsonpath='{.status.phase}' 2>/dev/null || echo "")
      if [[ "$phase" == "Terminating" ]]; then
        still_stuck+=("$ns")
      fi
    done

    if [[ ${#still_stuck[@]} -eq 0 ]]; then
      log_info "All namespaces deleted successfully."
      return
    fi

    stuck_namespaces=("${still_stuck[@]}")
    log_info "  ${#stuck_namespaces[@]} namespace(s) still terminating... (${elapsed}s/${timeout}s)"
    sleep "$interval"
    elapsed=$((elapsed + interval))
  done

  log_warn "${#stuck_namespaces[@]} namespace(s) stuck after ${timeout}s. Force-clearing finalizers..."
  for ns in "${stuck_namespaces[@]}"; do
    clear_namespace_finalizers "$ns"
  done
}

function cleanup_kubeconfig() {
  local context=$1
  local cluster_name=$2

  log_info "Cleaning up kubeconfig entries..."

  local oidc_user="oidc-$cluster_name"
  local oidc_context="oidc-$cluster_name"

  if command kubectl config get-contexts "$oidc_context" &>/dev/null; then
    command kubectl config delete-context "$oidc_context" 2>/dev/null || true
    log_info "  Deleted kubeconfig context: $oidc_context"
  fi

  if command kubectl config view -o jsonpath="{.users[?(@.name==\"$oidc_user\")].name}" 2>/dev/null | grep -qF "$oidc_user"; then
    command kubectl config delete-user "$oidc_user" 2>/dev/null || true
    log_info "  Deleted kubeconfig user: $oidc_user"
  fi

  if ! command kubectl config get-contexts "$context" &>/dev/null; then
    log_info "  Context '$context' not found in kubeconfig, skipping."
    return
  fi

  local ctx_cluster ctx_user
  ctx_cluster=$(command kubectl config view -o jsonpath="{.contexts[?(@.name==\"$context\")].context.cluster}" 2>/dev/null || echo "")
  ctx_user=$(command kubectl config view -o jsonpath="{.contexts[?(@.name==\"$context\")].context.user}" 2>/dev/null || echo "")

  command kubectl config delete-context "$context" 2>/dev/null || true
  log_info "  Deleted kubeconfig context: $context"

  if [[ -n "$ctx_cluster" ]]; then
    local other_refs
    other_refs=$(command kubectl config view -o jsonpath="{.contexts[?(@.context.cluster==\"$ctx_cluster\")].name}" 2>/dev/null || echo "")
    if [[ -z "$other_refs" ]]; then
      command kubectl config delete-cluster "$ctx_cluster" 2>/dev/null || true
      log_info "  Deleted kubeconfig cluster: $ctx_cluster"
    else
      log_info "  Kept kubeconfig cluster '$ctx_cluster' (still referenced by other contexts)"
    fi
  fi

  if [[ -n "$ctx_user" ]]; then
    local other_refs
    other_refs=$(command kubectl config view -o jsonpath="{.contexts[?(@.context.user==\"$ctx_user\")].name}" 2>/dev/null || echo "")
    if [[ -z "$other_refs" ]]; then
      command kubectl config delete-user "$ctx_user" 2>/dev/null || true
      log_info "  Deleted kubeconfig user: $ctx_user"
    else
      log_info "  Kept kubeconfig user '$ctx_user' (still referenced by other contexts)"
    fi
  fi

  local current
  current=$(command kubectl config current-context 2>/dev/null || echo "")
  if [[ "$current" == "$context" || "$current" == "$oidc_context" ]]; then
    command kubectl config unset current-context 2>/dev/null || true
    log_info "  Unset current-context (was pointing to deleted context)"
  fi
}

# ── Prerequisite Checks ───────────────────────────────────────────

function check_required_tools_init() {
  local required_tools=("kubectl" "helm" "htpasswd" "openssl")
  for tool in "${required_tools[@]}"; do
    if ! command -v "$tool" &> /dev/null; then
      log_error "$tool could not be found, please install it."
      exit 1
    fi
  done
}

function check_required_tools_deploy() {
  local required_tools=("kubectl" "helm" "yq" "git")
  for tool in "${required_tools[@]}"; do
    if ! command -v "$tool" &> /dev/null; then
      log_error "$tool could not be found, please install it."
      exit 1
    fi
  done
}

function check_required_tools_uninstall() {
  local required_tools=("kubectl" "helm")
  for tool in "${required_tools[@]}"; do
    if ! command -v "$tool" &> /dev/null; then
      log_error "$tool could not be found, please install it."
      exit 1
    fi
  done
}

function check_cluster_reachable() {
  local context=$1
  if ! kubectl cluster-info --context "$context" &>/dev/null; then
    log_error "Cannot connect to cluster using context '$context'."
    exit 1
  fi
}

function check_argocd_installed() {
  local context=$1
  if ! kubectl get namespace argocd --context "$context" &>/dev/null; then
    log_error "ArgoCD namespace not found. Run 'kr init' first to bootstrap the cluster."
    exit 1
  fi
  if ! helm status argocd --kube-context "$context" -n argocd &>/dev/null; then
    log_error "ArgoCD Helm release not found. Run 'kr init' first to bootstrap the cluster."
    exit 1
  fi
}

# ── Usage & Argument Parsing ──────────────────────────────────────

function usage() {
  cat <<EOF
Usage: kr <command> [flags]

Commands:
  up           Init (if needed) and deploy all clusters from kuberise.yaml
  version      Print kr version
  init         Bootstrap clusters (namespaces, secrets, CA, ArgoCD)
  deploy       Deploy all layers to already-bootstrapped clusters
  uninstall    Tear down the platform from a cluster

The recommended command is 'kr up' which automatically initializes
clusters that need it and deploys all layers.

Run 'kr <command> --help' for command-specific usage.
EOF
}

function usage_init() {
  cat <<EOF
Usage: kr init [flags]

Bootstrap Kubernetes clusters with kuberise platform prerequisites.
If kuberise.yaml exists, reads cluster config from it (context, domain, name).
Otherwise, requires --context and --domain flags for single-cluster mode.

Optional flags:
  --admin-password PWD     Admin password (default: admin, warns if default)
  --cilium                 Also install Cilium CNI
  --cluster NAME           Init only this cluster (when using kuberise.yaml)
  --config PATH            Path to kuberise.yaml (default: ./kuberise.yaml)
  -h, --help               Show this help message

Legacy single-cluster flags (used when --context is provided):
  --context CONTEXT        Kubernetes context name
  --domain DOMAIN          Base domain for services

Environment variables:
  ADMIN_PASSWORD           Admin password (alternative to --admin-password)
  GITEA_ADMIN_PASSWORD     Gitea admin password (default: value of admin password)
  CLOUDFLARE_API_TOKEN     Cloudflare API token for ExternalDNS and cert-manager
  OPENAI_API_KEY           OpenAI API key for K8sGPT

Examples:
  # Init all clusters from kuberise.yaml
  kr init

  # Init only the mgmt cluster
  kr init --cluster mgmt

  # Legacy: init a single cluster without kuberise.yaml
  kr init --context k3d-dev --domain dev.kuberise.dev
EOF
}

function usage_deploy() {
  cat <<EOF
Usage: kr deploy [flags]

Deploy all clusters and layers defined in kuberise.yaml.
Reads kuberise.yaml from the current directory (or --config path).
Deploys to all clusters in parallel, with retry for inaccessible clusters.

Required flags:
  --repo REPO_URL          Git repository URL for the client repo

Optional flags:
  --revision REV           Client repo branch, tag, or commit SHA (default: HEAD)
  --token TOKEN            Git token for private repositories (fallback for all repos)
  --cluster NAME           Deploy only this cluster (default: deploy all clusters)
  --layer NAME             Deploy only this layer (default: deploy all layers)
  --config PATH            Path to kuberise.yaml (default: ./kuberise.yaml)
  --retry-interval SECS    Seconds between retries for inaccessible clusters (default: 30)
  --retry-timeout SECS     Max seconds to wait for inaccessible clusters (default: 300)
  --dry-run                Show what would be applied without making changes
  -h, --help               Show this help message

kuberise.yaml defines kuberise version and a map of clusters with their layers:

  kuberise:
    repoURL: https://github.com/kuberise/kuberise.io.git
    targetRevision: 0.4.0

  clusters:
    mgmt:
      context: k3d-mgmt
      domain: mgmt.webshop.kuberise.dev
      layers:
        - name: platform
          repoURL: kuberise
        - name: pro
          repoURL: https://github.com/myorg/kuberise-pro.git
          targetRevision: main
    dev:
      context: k3d-dev
      domain: dev.webshop.kuberise.dev
      layers:
        - name: platform
          repoURL: kuberise
        - name: webshop

Each cluster's context is used for kubectl/helm operations. Enabler files
use the naming convention: values-{clusterName}-{layerName}.yaml

Examples:
  # Deploy all clusters and layers
  kr deploy --repo https://github.com/org/client-webshop.git

  # Deploy only the dev cluster
  kr deploy --repo https://github.com/org/client-webshop.git \\
    --cluster dev

  # Deploy only the platform layer on all clusters
  kr deploy --repo https://github.com/org/client-webshop.git \\
    --layer platform

  # Dry-run to preview generated manifests
  kr deploy --repo https://github.com/org/client-webshop.git \\
    --dry-run
EOF
}

function usage_uninstall() {
  cat <<EOF
Usage: kr uninstall [flags]

Uninstall kuberise platform from a Kubernetes cluster.
Removes all app-of-apps layers, ArgoCD, and kuberise-managed namespaces.

Required flags:
  --context CONTEXT        Kubernetes context name
  --cluster NAME           Cluster name (must match the name used during init)

Optional flags:
  --yes, -y                Skip interactive confirmation prompt
  -h, --help               Show this help message

Example:
  kr uninstall --context k3d-dev --cluster dev-app-onprem-one
EOF
}

# ── Parse: init ────────────────────────────────────────────────────

function parse_init_args() {
  INIT_CONTEXT=""
  INIT_CLUSTER="onprem"
  INIT_CLUSTER_FILTER=""
  INIT_DOMAIN=""
  INIT_ADMIN_PASSWORD=""
  INIT_CILIUM=false
  INIT_CONFIG="kuberise.yaml"

  while [[ $# -gt 0 ]]; do
    case "$1" in
      --context)          INIT_CONTEXT="$2";          shift 2 ;;
      --cluster)          INIT_CLUSTER_FILTER="$2";   shift 2 ;;
      --domain)           INIT_DOMAIN="$2";           shift 2 ;;
      --admin-password)   INIT_ADMIN_PASSWORD="$2";   shift 2 ;;
      --cilium)           INIT_CILIUM=true;           shift ;;
      --config)           INIT_CONFIG="$2";           shift 2 ;;
      --dry-run)          log_error "The init command does not support --dry-run. Only 'kr deploy' supports it."; exit 1 ;;
      --help|-h)          usage_init; exit 0 ;;
      *)                  log_error "Unknown option: $1"; usage_init; exit 1 ;;
    esac
  done

  INIT_ADMIN_PASSWORD="${INIT_ADMIN_PASSWORD:-${ADMIN_PASSWORD:-admin}}"

  # In legacy mode (explicit --context), use --cluster as the cluster name, not a filter
  if [[ -n "$INIT_CONTEXT" ]]; then
    INIT_CLUSTER="${INIT_CLUSTER_FILTER:-onprem}"
    INIT_CLUSTER_FILTER=""
  fi
}

function validate_init() {
  check_required_tools_init

  if [[ -z "$INIT_CONTEXT" ]]; then
    log_error "Missing required flag: --context"
    usage_init
    exit 1
  fi

  if [[ -z "$INIT_DOMAIN" ]]; then
    log_error "Missing required flag: --domain"
    usage_init
    exit 1
  fi

  check_cluster_reachable "$INIT_CONTEXT"

  if [[ "$INIT_ADMIN_PASSWORD" == "admin" ]]; then
    log_warn "Using default admin password 'admin'. Set --admin-password or ADMIN_PASSWORD env var for production use."
  fi
}

# ── Parse: deploy ──────────────────────────────────────────────────

function parse_deploy_args() {
  DEPLOY_REPO=""
  DEPLOY_REVISION="HEAD"
  DEPLOY_DRY_RUN=false
  DEPLOY_TOKEN=""
  DEPLOY_CLUSTER=""
  DEPLOY_LAYER=""
  DEPLOY_CONFIG="kuberise.yaml"
  DEPLOY_RETRY_INTERVAL=30
  DEPLOY_RETRY_TIMEOUT=300

  while [[ $# -gt 0 ]]; do
    case "$1" in
      --repo)             DEPLOY_REPO="$2";             shift 2 ;;
      --revision)         DEPLOY_REVISION="$2";         shift 2 ;;
      --token)            DEPLOY_TOKEN="$2";            shift 2 ;;
      --cluster)          DEPLOY_CLUSTER="$2";          shift 2 ;;
      --layer)            DEPLOY_LAYER="$2";            shift 2 ;;
      --config)           DEPLOY_CONFIG="$2";           shift 2 ;;
      --retry-interval)   DEPLOY_RETRY_INTERVAL="$2";   shift 2 ;;
      --retry-timeout)    DEPLOY_RETRY_TIMEOUT="$2";    shift 2 ;;
      --dry-run)          DEPLOY_DRY_RUN=true;          shift ;;
      --help|-h)          usage_deploy; exit 0 ;;
      *)                  log_error "Unknown option: $1"; usage_deploy; exit 1 ;;
    esac
  done
}

function validate_deploy() {
  check_required_tools_deploy

  if [[ -z "$DEPLOY_REPO" ]]; then
    log_error "Missing required flag: --repo"
    usage_deploy
    exit 1
  fi

  if [[ ! -f "$DEPLOY_CONFIG" ]]; then
    local fetched
    if fetched=$(fetch_kuberise_yaml "$DEPLOY_REPO" "$DEPLOY_REVISION" "$DEPLOY_TOKEN"); then
      DEPLOY_CONFIG="$fetched"
    else
      log_error "kuberise.yaml not found locally and could not be fetched from --repo."
      log_error "Run kr deploy from the root of your client repo, or use --config to specify the path."
      exit 1
    fi
  fi
}

# ── kuberise.yaml Parsing ───────────────────────────────────────────

function parse_kuberise_yaml() {
  local config_file=$1

  KUBERISE_REPO_URL=$(yq '.kuberise.repoURL' "$config_file")
  KUBERISE_TARGET_REVISION=$(yq '.kuberise.targetRevision' "$config_file")

  if [[ "$KUBERISE_REPO_URL" == "null" || -z "$KUBERISE_REPO_URL" ]]; then
    log_error "kuberise.repoURL is required in $config_file"
    exit 1
  fi
  if [[ "$KUBERISE_TARGET_REVISION" == "null" || -z "$KUBERISE_TARGET_REVISION" ]]; then
    log_error "kuberise.targetRevision is required in $config_file"
    exit 1
  fi

  # Read cluster names from the clusters map keys
  CLUSTER_NAMES=()
  while IFS= read -r name; do
    [[ -z "$name" || "$name" == "null" ]] && continue
    CLUSTER_NAMES+=("$name")
  done < <(yq '.clusters | keys | .[]' "$config_file")

  if [[ ${#CLUSTER_NAMES[@]} -eq 0 ]]; then
    log_error "At least one cluster must be defined in $config_file"
    exit 1
  fi
}

function get_cluster_config() {
  local config_file=$1
  local cluster_name=$2

  CL_CONTEXT=$(yq ".clusters.$cluster_name.context // \"\"" "$config_file")
  CL_DOMAIN=$(yq ".clusters.$cluster_name.domain" "$config_file")
  CL_DESTINATION=$(yq ".clusters.$cluster_name.destination // \"https://kubernetes.default.svc\"" "$config_file")
  CL_LAYERS_COUNT=$(yq ".clusters.$cluster_name.layers | length" "$config_file")

  if [[ "$CL_CONTEXT" == "null" || -z "$CL_CONTEXT" ]]; then
    log_error "clusters.$cluster_name.context is required in $config_file"
    exit 1
  fi
  if [[ "$CL_DOMAIN" == "null" || -z "$CL_DOMAIN" ]]; then
    log_error "clusters.$cluster_name.domain is required in $config_file"
    exit 1
  fi
  if [[ "$CL_LAYERS_COUNT" -eq 0 ]]; then
    log_error "At least one layer must be defined for cluster '$cluster_name' in $config_file"
    exit 1
  fi
}

function get_layer_config() {
  local config_file=$1
  local cluster_name=$2
  local index=$3

  LAYER_NAME=$(yq ".clusters.$cluster_name.layers[$index].name" "$config_file")
  local raw_repo
  raw_repo=$(yq ".clusters.$cluster_name.layers[$index].repoURL // \"\"" "$config_file")
  LAYER_TARGET_REVISION=$(yq ".clusters.$cluster_name.layers[$index].targetRevision // \"\"" "$config_file")
  LAYER_TOKEN_ENV=$(yq ".clusters.$cluster_name.layers[$index].token // \"\"" "$config_file")

  # Resolve repoURL
  if [[ "$raw_repo" == "kuberise" ]]; then
    LAYER_REPO_URL="$KUBERISE_REPO_URL"
    LAYER_TARGET_REVISION="${LAYER_TARGET_REVISION:-$KUBERISE_TARGET_REVISION}"
  elif [[ -n "$raw_repo" ]]; then
    LAYER_REPO_URL="$raw_repo"
  else
    # Default: use client repo
    LAYER_REPO_URL="$DEPLOY_REPO"
    LAYER_TARGET_REVISION="${LAYER_TARGET_REVISION:-$DEPLOY_REVISION}"
  fi

  # Ensure revision has a value
  if [[ -z "$LAYER_TARGET_REVISION" ]]; then
    LAYER_TARGET_REVISION="HEAD"
  fi

  # Resolve token from env var name
  LAYER_TOKEN=""
  if [[ -n "$LAYER_TOKEN_ENV" ]]; then
    LAYER_TOKEN="${!LAYER_TOKEN_ENV:-}"
    if [[ -z "$LAYER_TOKEN" ]]; then
      log_warn "Token env var '$LAYER_TOKEN_ENV' for layer '$LAYER_NAME' is not set or empty."
    fi
  fi
}

# ── Parse: uninstall ───────────────────────────────────────────────

function parse_uninstall_args() {
  UNINSTALL_CONTEXT=""
  UNINSTALL_CLUSTER=""
  UNINSTALL_ASSUME_YES=false

  while [[ $# -gt 0 ]]; do
    case "$1" in
      --context)      UNINSTALL_CONTEXT="$2";      shift 2 ;;
      --cluster)      UNINSTALL_CLUSTER="$2";       shift 2 ;;
      --yes|-y)       UNINSTALL_ASSUME_YES=true;    shift ;;
      --help|-h)      usage_uninstall; exit 0 ;;
      *)              log_error "Unknown option: $1"; usage_uninstall; exit 1 ;;
    esac
  done
}

function validate_uninstall() {
  check_required_tools_uninstall

  if [[ -z "$UNINSTALL_CONTEXT" ]]; then
    log_error "Missing required flag: --context"
    usage_uninstall
    exit 1
  fi

  if [[ -z "$UNINSTALL_CLUSTER" ]]; then
    log_error "Missing required flag: --cluster"
    usage_uninstall
    exit 1
  fi

  if ! kubectl cluster-info --context "$UNINSTALL_CONTEXT" &>/dev/null; then
    log_error "Cannot connect to cluster using context '$UNINSTALL_CONTEXT'."
    exit 1
  fi
}

# ── Subcommand: version ───────────────────────────────────────────

function cmd_version() {
  echo "kr version $KR_VERSION"
}

# ── Subcommand: init ──────────────────────────────────────────────

# Core init logic for a single cluster. Can be called by cmd_init or cmd_up.
function init_cluster() {
  local context=$1
  local cluster_name=$2
  local domain=$3
  local admin_password=$4
  local cilium=$5

  log_step "Initializing cluster: $cluster_name ($domain) via context $context"

  log_info "Creating namespaces..."
  create_all_namespaces "$context"

  log_info "Generating CA certificates..."
  generate_ca_cert_and_key "$context"

  log_info "Creating database secrets..."
  create_database_secrets "$context" "$admin_password"

  log_info "Creating application secrets..."
  create_application_secrets "$context" "$admin_password"

  if [[ "$cilium" == true ]]; then
    log_info "Installing Cilium..."
    install_cilium "$context"
  fi

  log_info "Installing ArgoCD..."
  install_argocd "$context" "$admin_password" "$domain"

  log_info "Configuring OAuth2 clients..."
  configure_oauth2_clients "$context" "$admin_password" "$domain" "$cluster_name"

  log_info "Cluster $cluster_name initialized successfully."
}

function cmd_init() {
  parse_init_args "$@"

  # If kuberise.yaml exists and no --context given, read clusters from it
  if [[ -z "$INIT_CONTEXT" && -f "$INIT_CONFIG" ]]; then
    check_required_tools_init
    check_required_tools_deploy  # need yq for kuberise.yaml parsing
    parse_kuberise_yaml "$INIT_CONFIG"

    if [[ "$INIT_ADMIN_PASSWORD" == "admin" ]]; then
      log_warn "Using default admin password 'admin'. Set --admin-password or ADMIN_PASSWORD env var for production use."
    fi

    local init_count=0
    for cluster_name in "${CLUSTER_NAMES[@]}"; do
      if [[ -n "$INIT_CLUSTER_FILTER" && "$cluster_name" != "$INIT_CLUSTER_FILTER" ]]; then
        continue
      fi

      get_cluster_config "$INIT_CONFIG" "$cluster_name"

      if ! kubectl cluster-info --context "$CL_CONTEXT" &>/dev/null; then
        log_warn "Cluster $cluster_name (context: $CL_CONTEXT) is not accessible. Skipping."
        continue
      fi

      init_cluster "$CL_CONTEXT" "$cluster_name" "$CL_DOMAIN" "$INIT_ADMIN_PASSWORD" "$INIT_CILIUM"
      init_count=$((init_count + 1))
    done

    echo ""
    if [[ "$init_count" -eq 0 ]]; then
      log_warn "No clusters initialized. Check --cluster filter or cluster accessibility."
    else
      log_info "$init_count cluster(s) initialized successfully."
      log_info "Next step: run 'kr deploy' or 'kr up' to deploy all layers."
    fi
  else
    # Legacy mode: explicit --context and --domain flags
    validate_init
    init_cluster "$INIT_CONTEXT" "$INIT_CLUSTER" "$INIT_DOMAIN" "$INIT_ADMIN_PASSWORD" "$INIT_CILIUM"
    echo ""
    log_info "Next step: run 'kr deploy' or 'kr up' from your client repo to deploy all layers."
  fi
}

# ── Subcommand: deploy ────────────────────────────────────────────

function deploy_cluster() {
  local config_file=$1
  local cluster_name=$2

  get_cluster_config "$config_file" "$cluster_name"

  local context="$CL_CONTEXT"
  local domain="$CL_DOMAIN"
  local destination="$CL_DESTINATION"
  local layers_count="$CL_LAYERS_COUNT"

  log_step "Deploying cluster: $cluster_name ($domain) via context $context"

  # Verify ArgoCD is installed on this cluster
  check_argocd_installed "$context"

  # Configure repo access for this cluster
  configure_all_repo_access "$context" \
    "$KUBERISE_REPO_URL" "$DEPLOY_TOKEN" \
    "$DEPLOY_REPO" "$DEPLOY_TOKEN"

  # Create ArgoCD project for this cluster
  create_app_project "$context" "$destination"

  local deployed_count=0
  for ((i=0; i<layers_count; i++)); do
    get_layer_config "$config_file" "$cluster_name" "$i"

    # Apply --layer filter if set
    if [[ -n "$DEPLOY_LAYER" && "$LAYER_NAME" != "$DEPLOY_LAYER" ]]; then
      continue
    fi

    log_info "  Layer: $LAYER_NAME ($LAYER_REPO_URL @ $LAYER_TARGET_REVISION)"

    # Configure layer-specific repo access if the layer has its own repo
    local layer_token="${LAYER_TOKEN:-$DEPLOY_TOKEN}"
    if [[ "$LAYER_REPO_URL" != "$KUBERISE_REPO_URL" && "$LAYER_REPO_URL" != "$DEPLOY_REPO" ]]; then
      configure_layer_repo_access "$context" "$LAYER_NAME" \
        "$LAYER_REPO_URL" "$layer_token"
    fi

    create_layer_app_of_apps "$context" "$cluster_name" "$LAYER_NAME" \
      "$LAYER_REPO_URL" "$LAYER_TARGET_REVISION" \
      "$KUBERISE_REPO_URL" "$KUBERISE_TARGET_REVISION" \
      "$DEPLOY_REPO" "$DEPLOY_REVISION" \
      "$domain" "$destination"

    deployed_count=$((deployed_count + 1))
  done

  if [[ "$deployed_count" -eq 0 ]]; then
    log_warn "Cluster $cluster_name: no layers deployed. Check --layer filter."
  else
    log_info "Cluster $cluster_name: $deployed_count layer(s) deployed."
  fi
}

function check_cluster_accessible() {
  local config_file=$1
  local cluster_name=$2

  local context
  context=$(yq ".clusters.$cluster_name.context // \"\"" "$config_file")

  if [[ -z "$context" || "$context" == "null" ]]; then
    return 1
  fi

  kubectl cluster-info --context "$context" &>/dev/null
}

function cmd_deploy() {
  parse_deploy_args "$@"
  validate_deploy
  parse_kuberise_yaml "$DEPLOY_CONFIG"

  KR_DRY_RUN="$DEPLOY_DRY_RUN"

  if [[ "$KR_DRY_RUN" == true ]]; then
    log_info "Dry-run mode: showing resources that would be applied (no changes will be made)"
  fi

  log_info "Kuberise: $KUBERISE_REPO_URL @ $KUBERISE_TARGET_REVISION"
  log_info "Clusters: ${CLUSTER_NAMES[*]}"

  # Determine which clusters to deploy (apply --cluster filter)
  local target_clusters=()
  for cluster_name in "${CLUSTER_NAMES[@]}"; do
    if [[ -n "$DEPLOY_CLUSTER" && "$cluster_name" != "$DEPLOY_CLUSTER" ]]; then
      continue
    fi
    target_clusters+=("$cluster_name")
  done

  if [[ ${#target_clusters[@]} -eq 0 ]]; then
    log_warn "No clusters matched. Check --cluster filter or kuberise.yaml."
    return
  fi

  # Separate accessible and inaccessible clusters
  local accessible=()
  local inaccessible=()
  for cluster_name in "${target_clusters[@]}"; do
    if check_cluster_accessible "$DEPLOY_CONFIG" "$cluster_name"; then
      accessible+=("$cluster_name")
    else
      inaccessible+=("$cluster_name")
    fi
  done

  if [[ ${#accessible[@]} -gt 0 ]]; then
    log_info "Accessible clusters: ${accessible[*]}"
  fi
  if [[ ${#inaccessible[@]} -gt 0 ]]; then
    log_warn "Inaccessible clusters (will retry): ${inaccessible[*]}"
  fi

  # Deploy accessible clusters in parallel
  local pids=()
  local log_files=()
  local cluster_for_pid=()

  for cluster_name in "${accessible[@]}"; do
    local log_file
    log_file=$(make_temp_file)
    log_files+=("$log_file")
    cluster_for_pid+=("$cluster_name")

    if [[ ${#accessible[@]} -eq 1 ]]; then
      # Single cluster: deploy inline (no background process needed)
      deploy_cluster "$DEPLOY_CONFIG" "$cluster_name"
    else
      # Multiple clusters: deploy in parallel
      deploy_cluster "$DEPLOY_CONFIG" "$cluster_name" > "$log_file" 2>&1 &
      pids+=($!)
    fi
  done

  # Wait for parallel deploys and collect results
  local failed_clusters=()
  if [[ ${#pids[@]} -gt 0 ]]; then
    for i in "${!pids[@]}"; do
      if wait "${pids[$i]}"; then
        log_info "Cluster ${cluster_for_pid[$i]}: deployment completed."
      else
        log_error "Cluster ${cluster_for_pid[$i]}: deployment failed."
        failed_clusters+=("${cluster_for_pid[$i]}")
      fi
      # Print the cluster's output
      if [[ -s "${log_files[$i]}" ]]; then
        echo "--- Output: ${cluster_for_pid[$i]} ---"
        cat "${log_files[$i]}"
        echo "--- End: ${cluster_for_pid[$i]} ---"
      fi
    done
  fi

  # Retry inaccessible clusters
  if [[ ${#inaccessible[@]} -gt 0 ]]; then
    local elapsed=0
    local remaining=("${inaccessible[@]}")

    while [[ ${#remaining[@]} -gt 0 && $elapsed -lt $DEPLOY_RETRY_TIMEOUT ]]; do
      log_info "Waiting ${DEPLOY_RETRY_INTERVAL}s before retrying inaccessible clusters... (${elapsed}s/${DEPLOY_RETRY_TIMEOUT}s)"
      sleep "$DEPLOY_RETRY_INTERVAL"
      elapsed=$((elapsed + DEPLOY_RETRY_INTERVAL))

      local still_waiting=()
      for cluster_name in "${remaining[@]}"; do
        if check_cluster_accessible "$DEPLOY_CONFIG" "$cluster_name"; then
          log_info "Cluster $cluster_name is now accessible. Deploying..."
          if deploy_cluster "$DEPLOY_CONFIG" "$cluster_name"; then
            log_info "Cluster $cluster_name: deployment completed."
          else
            log_error "Cluster $cluster_name: deployment failed."
            failed_clusters+=("$cluster_name")
          fi
        else
          still_waiting+=("$cluster_name")
        fi
      done
      remaining=("${still_waiting[@]}")
    done

    if [[ ${#remaining[@]} -gt 0 ]]; then
      log_error "Timed out waiting for clusters: ${remaining[*]}"
      failed_clusters+=("${remaining[@]}")
    fi
  fi

  # Summary
  echo ""
  if [[ "$KR_DRY_RUN" == true ]]; then
    log_info "Dry-run completed. No changes were made."
  elif [[ ${#failed_clusters[@]} -gt 0 ]]; then
    log_error "Deployment failed for clusters: ${failed_clusters[*]}"
    exit 1
  else
    log_info "All clusters deployed successfully."
    log_info "ArgoCD will now sync the applications defined in each app-of-apps."
  fi
}

# ── Subcommand: up ─────────────────────────────────────────────────

function is_argocd_installed() {
  local context=$1
  helm status argocd --kube-context "$context" -n argocd &>/dev/null
}

function usage_up() {
  cat <<EOF
Usage: kr up [flags]

Initialize and deploy all clusters defined in kuberise.yaml.
For each accessible cluster: bootstraps (init) if ArgoCD is not installed,
then deploys all layers. Clusters already initialized skip straight to deploy.

Required flags:
  --repo REPO_URL          Git repository URL for the client repo

Optional flags:
  --admin-password PWD     Admin password for init (default: admin, warns if default)
  --cilium                 Install Cilium CNI on clusters that need init
  --revision REV           Client repo branch, tag, or commit SHA (default: HEAD)
  --token TOKEN            Git token for private repositories (fallback for all repos)
  --cluster NAME           Target only this cluster (default: all clusters)
  --layer NAME             Deploy only this layer (default: deploy all layers)
  --config PATH            Path to kuberise.yaml (default: ./kuberise.yaml)
  --retry-interval SECS    Seconds between retries for inaccessible clusters (default: 30)
  --retry-timeout SECS     Max seconds to wait for inaccessible clusters (default: 300)
  --dry-run                Show what would be applied without making changes
  -h, --help               Show this help message

Examples:
  # Full setup: init + deploy all clusters
  kr up --repo https://github.com/org/client-webshop.git

  # Target only the dev cluster
  kr up --repo https://github.com/org/client-webshop.git --cluster dev

  # With Cilium and custom password
  kr up --repo https://github.com/org/client-webshop.git \\
    --cilium --admin-password mypassword
EOF
}

function parse_up_args() {
  UP_REPO=""
  UP_REVISION="HEAD"
  UP_TOKEN=""
  UP_CLUSTER=""
  UP_LAYER=""
  UP_CONFIG="kuberise.yaml"
  UP_ADMIN_PASSWORD=""
  UP_CILIUM=false
  UP_DRY_RUN=false
  UP_RETRY_INTERVAL=30
  UP_RETRY_TIMEOUT=300

  while [[ $# -gt 0 ]]; do
    case "$1" in
      --repo)             UP_REPO="$2";             shift 2 ;;
      --revision)         UP_REVISION="$2";         shift 2 ;;
      --token)            UP_TOKEN="$2";            shift 2 ;;
      --cluster)          UP_CLUSTER="$2";          shift 2 ;;
      --layer)            UP_LAYER="$2";            shift 2 ;;
      --config)           UP_CONFIG="$2";           shift 2 ;;
      --admin-password)   UP_ADMIN_PASSWORD="$2";   shift 2 ;;
      --cilium)           UP_CILIUM=true;           shift ;;
      --retry-interval)   UP_RETRY_INTERVAL="$2";   shift 2 ;;
      --retry-timeout)    UP_RETRY_TIMEOUT="$2";    shift 2 ;;
      --dry-run)          UP_DRY_RUN=true;          shift ;;
      --help|-h)          usage_up; exit 0 ;;
      *)                  log_error "Unknown option: $1"; usage_up; exit 1 ;;
    esac
  done

  UP_ADMIN_PASSWORD="${UP_ADMIN_PASSWORD:-${ADMIN_PASSWORD:-admin}}"
}

function validate_up() {
  # Check all tools needed for both init and deploy
  local required_tools=("kubectl" "helm" "htpasswd" "openssl" "yq" "git")
  for tool in "${required_tools[@]}"; do
    if ! command -v "$tool" &> /dev/null; then
      log_error "$tool could not be found, please install it."
      exit 1
    fi
  done

  if [[ -z "$UP_REPO" ]]; then
    log_error "Missing required flag: --repo"
    usage_up
    exit 1
  fi

  if [[ ! -f "$UP_CONFIG" ]]; then
    local fetched
    if fetched=$(fetch_kuberise_yaml "$UP_REPO" "$UP_REVISION" "$UP_TOKEN"); then
      UP_CONFIG="$fetched"
    else
      log_error "kuberise.yaml not found locally and could not be fetched from --repo."
      log_error "Run kr up from the root of your client repo, or use --config to specify the path."
      exit 1
    fi
  fi
}

function up_cluster() {
  local config_file=$1
  local cluster_name=$2

  get_cluster_config "$config_file" "$cluster_name"

  local context="$CL_CONTEXT"
  local domain="$CL_DOMAIN"
  local destination="$CL_DESTINATION"
  local layers_count="$CL_LAYERS_COUNT"

  # Check if ArgoCD is installed - if not, run init first
  if ! is_argocd_installed "$context"; then
    log_step "Cluster $cluster_name: ArgoCD not found, initializing..."
    if [[ "$UP_ADMIN_PASSWORD" == "admin" ]]; then
      log_warn "Using default admin password 'admin'. Set --admin-password or ADMIN_PASSWORD env var for production use."
    fi
    init_cluster "$context" "$cluster_name" "$domain" "$UP_ADMIN_PASSWORD" "$UP_CILIUM"
  else
    log_info "Cluster $cluster_name: ArgoCD already installed, skipping init."
  fi

  # Now deploy layers
  log_step "Deploying cluster: $cluster_name ($domain)"

  # Configure repo access
  configure_all_repo_access "$context" \
    "$KUBERISE_REPO_URL" "$UP_TOKEN" \
    "$UP_REPO" "$UP_TOKEN"

  # Create ArgoCD project
  create_app_project "$context" "$destination"

  local deployed_count=0
  for ((i=0; i<layers_count; i++)); do
    get_layer_config "$config_file" "$cluster_name" "$i"

    # Apply --layer filter if set
    if [[ -n "$UP_LAYER" && "$LAYER_NAME" != "$UP_LAYER" ]]; then
      continue
    fi

    log_info "  Layer: $LAYER_NAME ($LAYER_REPO_URL @ $LAYER_TARGET_REVISION)"

    # Configure layer-specific repo access if the layer has its own repo
    local layer_token="${LAYER_TOKEN:-$UP_TOKEN}"
    if [[ "$LAYER_REPO_URL" != "$KUBERISE_REPO_URL" && "$LAYER_REPO_URL" != "$UP_REPO" ]]; then
      configure_layer_repo_access "$context" "$LAYER_NAME" \
        "$LAYER_REPO_URL" "$layer_token"
    fi

    create_layer_app_of_apps "$context" "$cluster_name" "$LAYER_NAME" \
      "$LAYER_REPO_URL" "$LAYER_TARGET_REVISION" \
      "$KUBERISE_REPO_URL" "$KUBERISE_TARGET_REVISION" \
      "$UP_REPO" "$UP_REVISION" \
      "$domain" "$destination"

    deployed_count=$((deployed_count + 1))
  done

  if [[ "$deployed_count" -eq 0 ]]; then
    log_warn "Cluster $cluster_name: no layers deployed. Check --layer filter."
  else
    log_info "Cluster $cluster_name: $deployed_count layer(s) deployed."
  fi
}

function cmd_up() {
  parse_up_args "$@"
  validate_up
  parse_kuberise_yaml "$UP_CONFIG"

  # Set deploy globals used by get_layer_config
  DEPLOY_REPO="$UP_REPO"
  DEPLOY_REVISION="$UP_REVISION"
  DEPLOY_LAYER="$UP_LAYER"
  KR_DRY_RUN="$UP_DRY_RUN"

  if [[ "$KR_DRY_RUN" == true ]]; then
    log_info "Dry-run mode: showing resources that would be applied (no changes will be made)"
  fi

  log_info "Kuberise: $KUBERISE_REPO_URL @ $KUBERISE_TARGET_REVISION"
  log_info "Clusters: ${CLUSTER_NAMES[*]}"

  # Determine target clusters (apply --cluster filter)
  local target_clusters=()
  for cluster_name in "${CLUSTER_NAMES[@]}"; do
    if [[ -n "$UP_CLUSTER" && "$cluster_name" != "$UP_CLUSTER" ]]; then
      continue
    fi
    target_clusters+=("$cluster_name")
  done

  if [[ ${#target_clusters[@]} -eq 0 ]]; then
    log_warn "No clusters matched. Check --cluster filter or kuberise.yaml."
    return
  fi

  # Separate accessible and inaccessible clusters
  local accessible=()
  local inaccessible=()
  for cluster_name in "${target_clusters[@]}"; do
    if check_cluster_accessible "$UP_CONFIG" "$cluster_name"; then
      accessible+=("$cluster_name")
    else
      inaccessible+=("$cluster_name")
    fi
  done

  if [[ ${#accessible[@]} -gt 0 ]]; then
    log_info "Accessible clusters: ${accessible[*]}"
  fi
  if [[ ${#inaccessible[@]} -gt 0 ]]; then
    log_warn "Inaccessible clusters (will retry): ${inaccessible[*]}"
  fi

  # Process accessible clusters in parallel
  local pids=()
  local log_files=()
  local cluster_for_pid=()
  local failed_clusters=()

  for cluster_name in "${accessible[@]}"; do
    local log_file
    log_file=$(make_temp_file)
    log_files+=("$log_file")
    cluster_for_pid+=("$cluster_name")

    if [[ ${#accessible[@]} -eq 1 ]]; then
      up_cluster "$UP_CONFIG" "$cluster_name"
    else
      up_cluster "$UP_CONFIG" "$cluster_name" > "$log_file" 2>&1 &
      pids+=($!)
    fi
  done

  # Wait for parallel processes
  if [[ ${#pids[@]} -gt 0 ]]; then
    for i in "${!pids[@]}"; do
      if wait "${pids[$i]}"; then
        log_info "Cluster ${cluster_for_pid[$i]}: completed."
      else
        log_error "Cluster ${cluster_for_pid[$i]}: failed."
        failed_clusters+=("${cluster_for_pid[$i]}")
      fi
      if [[ -s "${log_files[$i]}" ]]; then
        echo "--- Output: ${cluster_for_pid[$i]} ---"
        cat "${log_files[$i]}"
        echo "--- End: ${cluster_for_pid[$i]} ---"
      fi
    done
  fi

  # Retry inaccessible clusters
  if [[ ${#inaccessible[@]} -gt 0 ]]; then
    local elapsed=0
    local remaining=("${inaccessible[@]}")

    while [[ ${#remaining[@]} -gt 0 && $elapsed -lt $UP_RETRY_TIMEOUT ]]; do
      log_info "Waiting ${UP_RETRY_INTERVAL}s before retrying inaccessible clusters... (${elapsed}s/${UP_RETRY_TIMEOUT}s)"
      sleep "$UP_RETRY_INTERVAL"
      elapsed=$((elapsed + UP_RETRY_INTERVAL))

      local still_waiting=()
      for cluster_name in "${remaining[@]}"; do
        if check_cluster_accessible "$UP_CONFIG" "$cluster_name"; then
          log_info "Cluster $cluster_name is now accessible."
          if up_cluster "$UP_CONFIG" "$cluster_name"; then
            log_info "Cluster $cluster_name: completed."
          else
            log_error "Cluster $cluster_name: failed."
            failed_clusters+=("$cluster_name")
          fi
        else
          still_waiting+=("$cluster_name")
        fi
      done
      remaining=("${still_waiting[@]}")
    done

    if [[ ${#remaining[@]} -gt 0 ]]; then
      log_error "Timed out waiting for clusters: ${remaining[*]}"
      failed_clusters+=("${remaining[@]}")
    fi
  fi

  # Summary
  echo ""
  if [[ "$KR_DRY_RUN" == true ]]; then
    log_info "Dry-run completed. No changes were made."
  elif [[ ${#failed_clusters[@]} -gt 0 ]]; then
    log_error "Failed clusters: ${failed_clusters[*]}"
    exit 1
  else
    log_info "All clusters are up and running."
    log_info "ArgoCD will now sync the applications defined in each app-of-apps."
  fi
}

# ── Subcommand: uninstall ─────────────────────────────────────────

function cmd_uninstall() {
  parse_uninstall_args "$@"
  validate_uninstall
  collect_uninstall_namespaces

  echo ""
  if [[ "$UNINSTALL_ASSUME_YES" != true ]]; then
    read -p "This will remove the '$UNINSTALL_CLUSTER' cluster installation from the '$UNINSTALL_CONTEXT' Kubernetes context.

It will:
  - Delete all app-of-apps ArgoCD applications and the project
  - Uninstall ArgoCD Helm release
  - Remove orphaned webhook configurations (cluster-scoped)
  - Delete kuberise-managed namespaces (including ArgoCD-discovered namespaces)
  - Remove cluster context and OIDC entries from kubeconfig (if present)
  - Cilium (CNI) will NOT be removed (the cluster needs it to function)

Are you sure you want to continue? (yes/no): " answer

    if [[ "$answer" != "yes" ]]; then
      echo "Aborting uninstallation."
      exit 0
    fi
  fi

  log_step "Removing app-of-apps"
  remove_app_of_apps

  log_step "Uninstalling ArgoCD"
  uninstall_argocd

  if [[ ${#KUBE_SYSTEM_HELM_RELEASES[@]} -gt 0 ]]; then
    log_step "Uninstalling kube-system Helm releases"
    uninstall_kube_system_releases
  fi

  log_step "Cleaning up cluster-scoped resources"
  cleanup_cluster_scoped_resources

  log_step "Deleting namespaces"
  delete_namespaces

  log_step "Checking for stuck namespaces"
  cleanup_stuck_namespaces

  log_step "Cleaning up kubeconfig"
  cleanup_kubeconfig "$UNINSTALL_CONTEXT" "$UNINSTALL_CLUSTER"

  echo ""
  log_info "kuberise uninstalled successfully."
}

# ── Main Dispatch ──────────────────────────────────────────────────

if [[ $# -eq 0 ]]; then
  usage
  exit 1
fi

case "$1" in
  up)         shift; cmd_up "$@" ;;
  version)    cmd_version ;;
  init)       shift; cmd_init "$@" ;;
  deploy)     shift; cmd_deploy "$@" ;;
  uninstall)  shift; cmd_uninstall "$@" ;;
  down)       shift; cmd_uninstall "$@" ;;
  --help|-h)  usage; exit 0 ;;
  *)          log_error "Unknown command: $1"; usage; exit 1 ;;
esac
